{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset): # inheritance \n",
    "\n",
    "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples):\n",
    "        self.annotations =  pd.read_csv(annotations_file) # pandas read the information in the csv file containing the annotations\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getimtem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        # load audio file\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        # maybe the signal can me a stereo signal, so we want it to be mono\n",
    "        # also the data could have different sample rate, we want to resample in order to have signals with the same one\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        # check if the num of samples is the same as the target one\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal) # equivalent to mel_spectrogram(signal)\n",
    "        return signal, label\n",
    "\n",
    "    # a_list[1] -> a_list.__getitem__(1)\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        # corresponds in the colon with index 5 in the csv\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\" # we get the number of fold in which the sample is from the annotation file .csv\n",
    "        # now join the path\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal) # callable object, very nice\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1: \n",
    "            # signal -> (num_channerls, samples) -> (2, 16000) -> (1, 16000) aggregate across dim = 0\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        if signal.shape[1] < self.num_samples:\n",
    "            diff = self.num_samples - signal.shape[1]\n",
    "            last_dim_padding = (0, diff) # left padding and right padding\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_FILE = \"...\"\n",
    "AUDIO_DIR = \"...\"\n",
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR)\n",
    "\n",
    "print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "\n",
    "signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft = 1024,\n",
    "    hop_length = 512,\n",
    "    n_mels = 64\n",
    ")\n",
    "\n",
    "# signal = mel_spectrogram(signal) callable object\n",
    "\n",
    "# new\n",
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE)\n",
    "signal, label = usd[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing audio with different durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data should be fixed in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 22050\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
